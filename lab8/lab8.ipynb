{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "763f5623",
   "metadata": {},
   "source": [
    "# Lab 8:  Question answering: Similarity based model training + conditional answering, simple RAG system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c67ca958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Who first landed on the Moon?\n",
      "A: The Apollo 11 mission landed the first humans on the Moon in 1969. \n",
      "\n",
      "Q: Tell me about Python programming.\n",
      "A: Python is a versatile programming language used for web development, data science, and automation. \n",
      "\n",
      "Q: Explain machine learning.\n",
      "A: Machine learning is a subset of artificial intelligence focused on building predictive models from data. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# -------------------------------\n",
    "# 1) Knowledge Base (Documents)\n",
    "# -------------------------------\n",
    "docs = [\n",
    "    \"The Apollo 11 mission landed the first humans on the Moon in 1969.\",\n",
    "    \"Python is a versatile programming language used for web development, data science, and automation.\",\n",
    "    \"The Great Wall of China is over 13,000 miles long and is one of the most famous landmarks in the world.\",\n",
    "    \"Machine learning is a subset of artificial intelligence focused on building predictive models from data.\"\n",
    "]\n",
    "\n",
    "# -------------------------------\n",
    "# 2) TF-IDF Vectorization\n",
    "# -------------------------------\n",
    "vectorizer = TfidfVectorizer()\n",
    "doc_vectors = vectorizer.fit_transform(docs)\n",
    "\n",
    "# -------------------------------\n",
    "# 3) Retrieval + Generation\n",
    "# -------------------------------\n",
    "def answer_question(query, top_k=1):\n",
    "    # Vectorize query\n",
    "    query_vec = vectorizer.transform([query])\n",
    "    \n",
    "    # Compute cosine similarity with all docs\n",
    "    sims = cosine_similarity(query_vec, doc_vectors)[0]\n",
    "    \n",
    "    # Get top-K most relevant documents\n",
    "    top_idxs = np.argsort(sims)[::-1][:top_k]\n",
    "    \n",
    "    # For this demo, we'll \"generate\" answer by returning top doc sentences\n",
    "    answers = [docs[i] for i in top_idxs]\n",
    "    \n",
    "    return answers\n",
    "\n",
    "# -------------------------------\n",
    "# 4) Try it out\n",
    "# -------------------------------\n",
    "query1 = \"Who first landed on the Moon?\"\n",
    "query2 = \"Tell me about Python programming.\"\n",
    "query3 = \"Explain machine learning.\"\n",
    "\n",
    "print(\"Q:\", query1)\n",
    "print(\"A:\", answer_question(query1)[0], \"\\n\")\n",
    "\n",
    "print(\"Q:\", query2)\n",
    "print(\"A:\", answer_question(query2)[0], \"\\n\")\n",
    "\n",
    "print(\"Q:\", query3)\n",
    "print(\"A:\", answer_question(query3)[0], \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982bbc0d",
   "metadata": {},
   "source": [
    "## Selecting Top-k Similar Documents\n",
    "\n",
    "In similarity-based information retrieval or question-answering systems, after computing similarity scores between a query and a set of documents, we often want to retrieve the top-k most relevant documents.\n",
    "\n",
    "### Step-by-step:\n",
    "\n",
    "1. **Compute Similarities**\n",
    "   - Use a similarity measure (e.g., cosine similarity) to compare the query vector with each document vector.\n",
    "   - Example:  \n",
    "     ```python\n",
    "     sims = [0.1, 0.8, 0.3, 0.5]  # similarity scores\n",
    "     ```\n",
    "\n",
    "2. **Sort Document Indices**\n",
    "   - `np.argsort(sims)` returns the indices that would sort the array in ascending order.  \n",
    "   - Example:  \n",
    "     ```python\n",
    "     np.argsort(sims)  # [0, 2, 3, 1]\n",
    "     ```\n",
    "\n",
    "3. **Reverse for Descending Order**\n",
    "   - Using `[::-1]` reverses the array to get descending order (highest similarity first).  \n",
    "   - Example:  \n",
    "     ```python\n",
    "     np.argsort(sims)[::-1]  # [1, 3, 2, 0]\n",
    "     ```\n",
    "\n",
    "4. **Select Top-k**\n",
    "   - Slice the first `k` indices to get the most similar documents.  \n",
    "   - Example (`top_k = 2`):  \n",
    "     ```python\n",
    "     top_idxs = [1, 3]  # indices of the top 2 documents\n",
    "     ```\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **Purpose:** Retrieve the most relevant documents efficiently.\n",
    "- **Key Function:**  \n",
    "  ```python\n",
    "  top_idxs = np.argsort(sims)[::-1][:top_k]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
